{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Noting:\n",
    "## Camera Depth\n",
    "Closer Person and Farther Person from the camera lead to different skeletons coordinate making the action detection model prone to changes in 3D environment. Since joint locations are detected in pixel coordinates, a person who is far away will have joint coordinates that appear compressed, while those who are closer will appear expanded.  \n",
    "-> Solution: Normalize Person Coordinate and calculate joint distances.\n",
    "+ Joint locations are Normalized using equation (1) where $(x_i, y_i)$ and $(x'_i, y'_i)$ are the original joint coordinate and normalized joint coordinate in i-th position. Thus the normalized joints reprepresent n features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import mediapipe as mp\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Image Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH = 640\n",
    "HEIGHT = 640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_and_show(img):\n",
    "    h, w = img.shape[:2]\n",
    "    if h < w:\n",
    "        img = cv.resize(img, WIDTH, HEIGHT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Make Detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils # Drawing helpers\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_holistic = mp.solutions.holistic # Mediapipe Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holistic Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv.VideoCapture(0)\n",
    "\n",
    "# Initiate holistic model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        #? Detect and Render\n",
    "        # Recolor image (OpenCV use BGR, convert to RGB for processing)\n",
    "        image = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False # block image writing capability  \n",
    "        \n",
    "        # Make Detections\n",
    "        results = holistic.process(image)\n",
    "        # print(results.face_landmarks)\n",
    "        \n",
    "        # face_landmarks, pose_landmarks, left_hand_landmarks, right_hand_landmarks\n",
    "        \n",
    "        #? Recolor image back to BGR for rendering\n",
    "        image.flags.writeable = True   \n",
    "        image = cv.cvtColor(image, cv.COLOR_RGB2BGR)\n",
    "        \n",
    "        # 1. Draw face landmarks\n",
    "        # mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "        #                          mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "        #                          mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "        #                          )\n",
    "        \n",
    "        # 2. Right hand\n",
    "        # mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "        #                          mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "        #                          mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "        #                          )\n",
    "\n",
    "        # 3. Left Hand\n",
    "        # mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "        #                          mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "        #                          mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "        #                          )\n",
    "\n",
    "        # 4. Pose Detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "                        \n",
    "        cv.imshow('Raw Webcam Feed', image)\n",
    "\n",
    "        if cv.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9935827851295471"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.pose_landmarks.landmark[0].visibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# New variable name for the holistic model\n",
    "holistic_model = mp.solutions.holistic\n",
    "drawing_utils = mp.solutions.drawing_utils\n",
    "\n",
    "# Specify the path to your image folder\n",
    "image_folder = 'action_image'  # Replace with the actual path to your folder\n",
    "\n",
    "# Get a list of all files in the image folder\n",
    "image_files = [f for f in os.listdir(image_folder) if f.endswith(('.jpg', '.jpeg', '.png'))] # You can add more image extensions if needed\n",
    "\n",
    "# Initiate holistic model\n",
    "with holistic_model.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "\n",
    "    for image_file in image_files:\n",
    "        # Construct the full path to the image\n",
    "        image_path = os.path.join(image_folder, image_file)\n",
    "\n",
    "        # Read the image using OpenCV\n",
    "        frame = cv.imread(image_path)\n",
    "\n",
    "        # Check if the image was successfully read\n",
    "        if frame is None:\n",
    "            print(f\"Could not read image: {image_path}\")\n",
    "            continue\n",
    "\n",
    "        #? Detect and Render\n",
    "        # Recolor image (OpenCV use BGR, convert to RGB for processing)\n",
    "        image = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False # block image writing capability\n",
    "\n",
    "        # Make Detections\n",
    "        results_test = holistic.process(image)\n",
    "        # print(results_test.face_landmarks)\n",
    "\n",
    "        # face_landmarks, pose_landmarks, left_hand_landmarks, right_hand_landmarks\n",
    "\n",
    "        #? Recolor image back to BGR for rendering\n",
    "        image.flags.writeable = True\n",
    "        image = cv.cvtColor(image, cv.COLOR_RGB2BGR)\n",
    "\n",
    "        # Pose Detections\n",
    "        drawing_utils.draw_landmarks(image, results_test.pose_landmarks, holistic_model.POSE_CONNECTIONS,\n",
    "                                         drawing_utils.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                                         drawing_utils.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                                         )\n",
    "\n",
    "        # Display the processed image\n",
    "        cv.imshow(f'Processed Image: {image_file}', image)\n",
    "\n",
    "        # Wait for a key press for each image (optional)\n",
    "        cv.waitKey(0)\n",
    "\n",
    "# Destroy all windows after processing all images\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9987233281135559"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test.pose_landmarks.landmark[0].visibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Capture Landmarks & Export to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/3j8BPdc.png\" style=\"height:300px\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_coords = len(results.pose_landmarks.landmark)\n",
    "num_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks = ['class']\n",
    "for val in range(1, num_coords+1):\n",
    "    landmarks += ['x{}'.format(val), 'y{}'.format(val), 'z{}'.format(val), 'v{}'.format(val)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('coords.csv', mode='w', newline='') as f:\n",
    "    csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    csv_writer.writerow(landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name = \"assault_violence\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import mediapipe as mp\n",
    "import os\n",
    "\n",
    "# New variable name for the holistic model\n",
    "holistic_model = mp.solutions.holistic\n",
    "drawing_utils = mp.solutions.drawing_utils\n",
    "\n",
    "# Specify the path to your image folder\n",
    "image_folder = 'action_image'  # Replace with the actual path to your folder\n",
    "\n",
    "# Get a list of all files in the image folder\n",
    "image_files = [f for f in os.listdir(image_folder) if f.endswith(('.jpg', '.jpeg', '.png'))] # You can add more image extensions if needed\n",
    "\n",
    "# Initiate holistic model\n",
    "with holistic_model.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "\n",
    "    for image_file in image_files:\n",
    "        # Construct the full path to the image\n",
    "        image_path = os.path.join(image_folder, image_file)\n",
    "\n",
    "        # Read the image using OpenCV\n",
    "        frame = cv.imread(image_path)\n",
    "\n",
    "        # Check if the image was successfully read\n",
    "        if frame is None:\n",
    "            print(f\"Could not read image: {image_path}\")\n",
    "            continue\n",
    "\n",
    "        #? Detect and Render\n",
    "        # Recolor image (OpenCV use BGR, convert to RGB for processing)\n",
    "        image = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False # block image writing capability\n",
    "\n",
    "        # Make Detections\n",
    "        results_test = holistic.process(image)\n",
    "        # print(results_test.face_landmarks)\n",
    "\n",
    "        # face_landmarks, pose_landmarks, left_hand_landmarks, right_hand_landmarks\n",
    "\n",
    "        #? Recolor image back to BGR for rendering\n",
    "        image.flags.writeable = True\n",
    "        image = cv.cvtColor(image, cv.COLOR_RGB2BGR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New variable name for the holistic model\n",
    "holistic_model = mp.solutions.holistic\n",
    "drawing_utils = mp.solutions.drawing_utils\n",
    "\n",
    "# Specify the path to your image folder\n",
    "image_folder = 'image_frames'  # Replace with the actual path to your folder\n",
    "\n",
    "# Get a list of all files in the image folder\n",
    "image_files = [f for f in os.listdir(image_folder) if f.endswith(('.jpg', '.jpeg', '.png'))] # You can add more image extensions if needed\n",
    "\n",
    "# Initiate holistic model\n",
    "with holistic_model.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "\n",
    "    for image_file in image_files:\n",
    "        # Construct the full path to the image\n",
    "        image_path = os.path.join(image_folder, image_file)\n",
    "\n",
    "        # Read the image using OpenCV\n",
    "        frame = cv.imread(image_path)\n",
    "\n",
    "        # Check if the image was successfully read\n",
    "        if frame is None:\n",
    "            print(f\"Could not read image: {image_path}\")\n",
    "            continue\n",
    "\n",
    "        #? Detect and Render\n",
    "        # Recolor image (OpenCV use BGR, convert to RGB for processing)\n",
    "        image = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False # block image writing capability\n",
    "\n",
    "        # Make Detections\n",
    "        results_test = holistic.process(image)\n",
    "        # print(results_test.face_landmarks)\n",
    "\n",
    "        # face_landmarks, pose_landmarks, left_hand_landmarks, right_hand_landmarks\n",
    "\n",
    "        #? Recolor image back to BGR for rendering\n",
    "        image.flags.writeable = True\n",
    "        image = cv.cvtColor(image, cv.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Pose Detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Export coordinates\n",
    "        try:\n",
    "            # Extract Pose landmarks\n",
    "            pose = results.pose_landmarks.landmark\n",
    "            pose_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in pose]).flatten())\n",
    "            \n",
    "            # Concate rows\n",
    "            row = pose_row\n",
    "            \n",
    "            # Append class name \n",
    "            row.insert(0, class_name)\n",
    "            \n",
    "            # Export to CSV\n",
    "            with open('coords.csv', mode='a', newline='') as f:\n",
    "                csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "                csv_writer.writerow(row) \n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "                        \n",
    "        cv.imshow('Raw Webcam Feed', image)\n",
    "\n",
    "        if cv.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Read in Collected Data and Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('coords.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['class']=='assault_violence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('class', axis=1) # features\n",
    "y = df['class'] # target value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Train Machine Learning Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = {\n",
    "    'lr':make_pipeline(StandardScaler(), LogisticRegression()),\n",
    "    'rc':make_pipeline(StandardScaler(), RidgeClassifier()),\n",
    "    'rf':make_pipeline(StandardScaler(), RandomForestClassifier()),\n",
    "    'gb':make_pipeline(StandardScaler(), GradientBoostingClassifier()),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_models = {}\n",
    "for algo, pipeline in pipelines.items():\n",
    "    model = pipeline.fit(X_train, y_train)\n",
    "    fit_models[algo] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_models['rc'].predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Evaluate and Serialize Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediapipe_pose",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
