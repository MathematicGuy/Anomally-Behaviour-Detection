{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Noting:\n",
    "## Camera Depth\n",
    "Closer Person and Farther Person from the camera lead to different skeletons coordinate making the action detection model prone to changes in 3D environment. Since joint locations are detected in pixel coordinates, a person who is far away will have joint coordinates that appear compressed, while those who are closer will appear expanded.  \n",
    "-> Solution: Normalize Person Coordinate and calculate joint distances.\n",
    "+ Joint locations are Normalized using equation (1) where $(x_i, y_i)$ and $(x'_i, y'_i)$ are the original joint coordinate and normalized joint coordinate in i-th position. Thus the normalized joints reprepresent n features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import cv2 as cv\n",
    "import pandas as pd\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading a Video and Saving Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import os\n",
    "\n",
    "def video_to_frames(video_path, output_folder, skip_rate=5):\n",
    "    \"\"\"\n",
    "    Extract frames from a video and save them as .jpg files.\n",
    "    skip_rate: Save 1 frame every 'skip_rate' frames to reduce data size if needed.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    cap = cv.VideoCapture(video_path)\n",
    "    frame_count = 0\n",
    "    saved_count = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Save every 'skip_rate' frames (optional)\n",
    "        if frame_count % skip_rate == 0:\n",
    "            frame_filename = f\"frame_{frame_count}.jpg\"\n",
    "            frame_path = os.path.join(output_folder, frame_filename)\n",
    "            cv.imwrite(frame_path, frame)\n",
    "            saved_count += 1\n",
    "        \n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    print(f\"Saved {saved_count} frames from {video_path} to {output_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through all video to extract frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating directory 'frames' for extracted frames.\n",
      "Saved 9 frames from Single_person_violent\\Kicking\\kicking1.mp4 to frames\\Kicking\\kicking1\n",
      "Saved 15 frames from Single_person_violent\\Kicking\\kicking10.mp4 to frames\\Kicking\\kicking10\n",
      "Saved 12 frames from Single_person_violent\\Kicking\\kicking11.mp4 to frames\\Kicking\\kicking11\n",
      "Saved 16 frames from Single_person_violent\\Kicking\\kicking12.mp4 to frames\\Kicking\\kicking12\n",
      "Saved 5 frames from Single_person_violent\\Kicking\\kicking13.mp4 to frames\\Kicking\\kicking13\n",
      "Saved 7 frames from Single_person_violent\\Kicking\\kicking14.mp4 to frames\\Kicking\\kicking14\n",
      "Saved 8 frames from Single_person_violent\\Kicking\\kicking15.mp4 to frames\\Kicking\\kicking15\n",
      "Saved 6 frames from Single_person_violent\\Kicking\\kicking16.mp4 to frames\\Kicking\\kicking16\n",
      "Saved 8 frames from Single_person_violent\\Kicking\\kicking17.mp4 to frames\\Kicking\\kicking17\n",
      "Saved 17 frames from Single_person_violent\\Kicking\\kicking18.mp4 to frames\\Kicking\\kicking18\n",
      "Saved 15 frames from Single_person_violent\\Kicking\\kicking19.mp4 to frames\\Kicking\\kicking19\n",
      "Saved 13 frames from Single_person_violent\\Kicking\\kicking2.mp4 to frames\\Kicking\\kicking2\n",
      "Saved 18 frames from Single_person_violent\\Kicking\\kicking20.mp4 to frames\\Kicking\\kicking20\n",
      "Saved 45 frames from Single_person_violent\\Kicking\\kicking21.mp4 to frames\\Kicking\\kicking21\n",
      "Saved 10 frames from Single_person_violent\\Kicking\\kicking3.mp4 to frames\\Kicking\\kicking3\n",
      "Saved 9 frames from Single_person_violent\\Kicking\\kicking4.mp4 to frames\\Kicking\\kicking4\n",
      "Saved 14 frames from Single_person_violent\\Kicking\\kicking5.mp4 to frames\\Kicking\\kicking5\n",
      "Saved 20 frames from Single_person_violent\\Kicking\\kicking6.mp4 to frames\\Kicking\\kicking6\n",
      "Saved 22 frames from Single_person_violent\\Kicking\\kicking7.mp4 to frames\\Kicking\\kicking7\n",
      "Saved 19 frames from Single_person_violent\\Kicking\\kicking8.mp4 to frames\\Kicking\\kicking8\n",
      "Saved 15 frames from Single_person_violent\\Kicking\\kicking9.mp4 to frames\\Kicking\\kicking9\n",
      "Saved 6 frames from Single_person_violent\\Punching\\punching1.mp4 to frames\\Punching\\punching1\n",
      "Saved 19 frames from Single_person_violent\\Punching\\punching10.mp4 to frames\\Punching\\punching10\n",
      "Saved 11 frames from Single_person_violent\\Punching\\punching11.mp4 to frames\\Punching\\punching11\n",
      "Saved 12 frames from Single_person_violent\\Punching\\punching12.mp4 to frames\\Punching\\punching12\n",
      "Saved 6 frames from Single_person_violent\\Punching\\punching13.mp4 to frames\\Punching\\punching13\n",
      "Saved 12 frames from Single_person_violent\\Punching\\punching14.mp4 to frames\\Punching\\punching14\n",
      "Saved 8 frames from Single_person_violent\\Punching\\punching15.mp4 to frames\\Punching\\punching15\n",
      "Saved 9 frames from Single_person_violent\\Punching\\punching16.mp4 to frames\\Punching\\punching16\n",
      "Saved 13 frames from Single_person_violent\\Punching\\punching17.mp4 to frames\\Punching\\punching17\n",
      "Saved 9 frames from Single_person_violent\\Punching\\punching18.mp4 to frames\\Punching\\punching18\n",
      "Saved 7 frames from Single_person_violent\\Punching\\punching19.mp4 to frames\\Punching\\punching19\n",
      "Saved 9 frames from Single_person_violent\\Punching\\punching2.mp4 to frames\\Punching\\punching2\n",
      "Saved 6 frames from Single_person_violent\\Punching\\punching20.mp4 to frames\\Punching\\punching20\n",
      "Saved 3 frames from Single_person_violent\\Punching\\punching21.mp4 to frames\\Punching\\punching21\n",
      "Saved 6 frames from Single_person_violent\\Punching\\punching22.mp4 to frames\\Punching\\punching22\n",
      "Saved 6 frames from Single_person_violent\\Punching\\punching23.mp4 to frames\\Punching\\punching23\n",
      "Saved 3 frames from Single_person_violent\\Punching\\punching24.mp4 to frames\\Punching\\punching24\n",
      "Saved 8 frames from Single_person_violent\\Punching\\punching25.mp4 to frames\\Punching\\punching25\n",
      "Saved 6 frames from Single_person_violent\\Punching\\punching26.mp4 to frames\\Punching\\punching26\n",
      "Saved 9 frames from Single_person_violent\\Punching\\punching27.mp4 to frames\\Punching\\punching27\n",
      "Saved 8 frames from Single_person_violent\\Punching\\punching28.mp4 to frames\\Punching\\punching28\n",
      "Saved 8 frames from Single_person_violent\\Punching\\punching29.mp4 to frames\\Punching\\punching29\n",
      "Saved 11 frames from Single_person_violent\\Punching\\Punching3.mp4 to frames\\Punching\\Punching3\n",
      "Saved 5 frames from Single_person_violent\\Punching\\punching30.mp4 to frames\\Punching\\punching30\n",
      "Saved 25 frames from Single_person_violent\\Punching\\punching31.mp4 to frames\\Punching\\punching31\n",
      "Saved 4 frames from Single_person_violent\\Punching\\punching32.mp4 to frames\\Punching\\punching32\n",
      "Saved 8 frames from Single_person_violent\\Punching\\punching4.mp4 to frames\\Punching\\punching4\n",
      "Saved 9 frames from Single_person_violent\\Punching\\punching5.mp4 to frames\\Punching\\punching5\n",
      "Saved 12 frames from Single_person_violent\\Punching\\punching6.mp4 to frames\\Punching\\punching6\n",
      "Saved 14 frames from Single_person_violent\\Punching\\punching7.mp4 to frames\\Punching\\punching7\n",
      "Saved 15 frames from Single_person_violent\\Punching\\punching8.mp4 to frames\\Punching\\punching8\n",
      "Saved 17 frames from Single_person_violent\\Punching\\punching9.mp4 to frames\\Punching\\punching9\n",
      "Saved 3 frames from Single_person_violent\\Non-violent\\non1.mp4 to frames\\Non-violent\\non1\n",
      "Saved 8 frames from Single_person_violent\\Non-violent\\non2.mp4 to frames\\Non-violent\\non2\n",
      "Saved 5 frames from Single_person_violent\\Non-violent\\non3.mp4 to frames\\Non-violent\\non3\n",
      "Saved 16 frames from Single_person_violent\\Non-violent\\non4.mp4 to frames\\Non-violent\\non4\n",
      "Saved 7 frames from Single_person_violent\\Non-violent\\non5.mp4 to frames\\Non-violent\\non5\n",
      "The directory 'frames' already exists. Skipping frame extraction.\n"
     ]
    }
   ],
   "source": [
    "frame_output_root = \"frames\" # where extracted frames go\n",
    "\n",
    "import glob\n",
    "\n",
    "base_dir = \"Single_person_violent\"\n",
    "classes = [\"Kicking\", \"Punching\", \"Non-violent\"]\n",
    "\n",
    "X_sequences = []  # list of arrays, each array is shape (T, 132)\n",
    "y_labels = []     # matching list of string labels\n",
    "\n",
    "if not os.path.exists(frame_output_root):\n",
    "    print(f\"Creating directory '{frame_output_root}' for extracted frames.\")\n",
    "\n",
    "for cls in classes:\n",
    "    class_video_dir = os.path.join(base_dir, cls)\n",
    "    for video_file in os.listdir(class_video_dir):\n",
    "        if video_file.endswith(\".mp4\") or video_file.endswith(\".avi\"):\n",
    "            video_path = os.path.join(class_video_dir, video_file)\n",
    "            # Output folder for frames of THIS video\n",
    "            output_folder = os.path.join(frame_output_root, cls, video_file.split('.')[0])\n",
    "            video_to_frames(video_path, output_folder, skip_rate=5)\n",
    "\n",
    "else:\n",
    "    print(f\"The directory '{frame_output_root}' already exists. Skipping frame extraction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resize Image for Training & Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 as cv\n",
    "\n",
    "def resize_images(image_folder, max_width=640, max_height=640):\n",
    "    \"\"\"\n",
    "    Resizes all images in the given folder to the specified width and height,\n",
    "    maintaining aspect ratio.\n",
    "    \"\"\"\n",
    "    # Get a list of all files in the image folder\n",
    "    image_files = [f for f in os.listdir(image_folder) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "    for image_file in image_files:\n",
    "        image_path = os.path.join(image_folder, image_file)\n",
    "        \n",
    "        # Read the image using OpenCV\n",
    "        img = cv.imread(image_path)\n",
    "        \n",
    "        # Check if the image was successfully read\n",
    "        if img is None:\n",
    "            print(f\"Could not read image: {image_path}\")\n",
    "            continue\n",
    "        \n",
    "        height, width = img.shape[:2]\n",
    "\n",
    "        # Calculate the scaling factor\n",
    "        width_scale = max_width / width\n",
    "        height_scale = max_height / height\n",
    "        scale = min(width_scale, height_scale)  # Choose the smaller scale to fit within both dimensions\n",
    "\n",
    "        # Calculate the new dimensions\n",
    "        new_width = int(width * scale)\n",
    "        new_height = int(height * scale)\n",
    "        \n",
    "        # Resize the image\n",
    "        resized_img = cv.resize(img, (new_width, new_height), interpolation=cv.INTER_AREA)\n",
    "        \n",
    "        # Save the resized image, overwriting the original\n",
    "        cv.imwrite(image_path, resized_img)\n",
    "        \n",
    "        print(f\"Resized {image_file} to {new_width}x{new_height}\")\n",
    "\n",
    "# Example usage:\n",
    "# resize_images('image_frames')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pose Detection & Export Estimation Data to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up tools and import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "csv_filename = 'pose_landmarks.csv'\n",
    "num_landmarks = 33  # MediaPipe Pose has 33 landmarks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Write CSV Header "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ['class_label']  # e.g. Kick, Punching, Non-violent\n",
    "for i in range(num_landmarks):\n",
    "    header += [f'x{i}', f'y{i}', f'z{i}', f'v{i}']\n",
    "\n",
    "# Initialize CSV file\n",
    "with open(csv_filename, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Extract Landmarks Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each frame has 33 landmarks (MediaPipe Pose detects 33 distinct body keypoints). For each landmark, you get 4 values:\n",
    "\n",
    "+ x (normalized horizontal coordinate)\n",
    "\n",
    "+ y (normalized vertical coordinate)\n",
    "\n",
    "+ z (relative depth)\n",
    "\n",
    "+ visibility (confidence of that landmark being visible)\n",
    "\n",
    "So for each frame: \n",
    "+ 33 landmarks × 4 values each = 132 features total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pose_landmarks(image_path, label, pose_estimator, csv_file):\n",
    "    frame = cv.imread(image_path)\n",
    "    if frame is None:\n",
    "        print(f\"Could not read {image_path}\")\n",
    "        return\n",
    "    \n",
    "    # Convert BGR to RGB for MediaPipe\n",
    "    rgb_image = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "    rgb_image.flags.writeable = False\n",
    "\n",
    "    # Process\n",
    "    results = pose_estimator.process(rgb_image)\n",
    "\n",
    "    if not results.pose_landmarks:\n",
    "        return  # No detection, skip\n",
    "\n",
    "    # Flatten the 33 landmarks\n",
    "    pose_row = []\n",
    "    for lm in results.pose_landmarks.landmark:\n",
    "        pose_row.extend([lm.x, lm.y, lm.z, lm.visibility])\n",
    "    \n",
    "    row = [label] + pose_row\n",
    "\n",
    "    # Write to CSV\n",
    "    with open(csv_file, 'a', newline='') as f_out:\n",
    "        writer = csv.writer(f_out)\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Main Loop to Process All Frames using Mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'frames\\\\Kick'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m class_frame_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# subfolders: e.g. \"kicking1\", \"kicking2\"\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m video_folder \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_frame_dir\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     14\u001b[0m     video_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(class_frame_dir, video_folder)\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(video_path):\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'frames\\\\Kick'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2 as cv\n",
    "\n",
    "# Use a \"with\" statement to keep the Pose model open\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, \n",
    "                  min_tracking_confidence=0.5) as pose_estimator:\n",
    "    \n",
    "    # We revisit the 'frames' folder structure\n",
    "    for cls in classes:  # Kick, Punching, Non-violent\n",
    "        class_frame_dir = os.path.join(\"frames\", cls)\n",
    "        \n",
    "        # subfolders: e.g. \"kicking1\", \"kicking2\"\n",
    "        for video_folder in os.listdir(class_frame_dir):\n",
    "            video_path = os.path.join(class_frame_dir, video_folder)\n",
    "            if not os.path.isdir(video_path):\n",
    "                continue\n",
    "            \n",
    "            # each file is a frame\n",
    "            frame_files = [f for f in os.listdir(video_path) \n",
    "                           if f.lower().endswith(('.jpg', '.png'))]\n",
    "            frame_files.sort()\n",
    "            \n",
    "            for frame_file in frame_files:\n",
    "                frame_path = os.path.join(video_path, frame_file)\n",
    "                extract_pose_landmarks(frame_path, cls, pose_estimator, csv_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When this loop finishes, pose_landmarks.csv will have one row per frame, with 133 columns: `[class_label, x0, y0, z0, v0, x1, y1, z1, v1, ..., x32, y32, z32, v32]`.\n",
    "\n",
    "class_label is “Kick,” “Punching,” or “Non-violent,” depending on the folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/3j8BPdc.png\" style=\"height:300px\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset Evaluattion and Split Dataset for Train and Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>x0</th>\n",
       "      <td>1557.0</td>\n",
       "      <td>0.270993</td>\n",
       "      <td>0.037887</td>\n",
       "      <td>0.160436</td>\n",
       "      <td>0.248814</td>\n",
       "      <td>0.274044</td>\n",
       "      <td>0.293484</td>\n",
       "      <td>0.474454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y0</th>\n",
       "      <td>1557.0</td>\n",
       "      <td>0.432748</td>\n",
       "      <td>0.147763</td>\n",
       "      <td>-0.112364</td>\n",
       "      <td>0.330139</td>\n",
       "      <td>0.455499</td>\n",
       "      <td>0.542123</td>\n",
       "      <td>0.862946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>z0</th>\n",
       "      <td>1557.0</td>\n",
       "      <td>-0.201858</td>\n",
       "      <td>0.156447</td>\n",
       "      <td>-1.084517</td>\n",
       "      <td>-0.301598</td>\n",
       "      <td>-0.203921</td>\n",
       "      <td>-0.084499</td>\n",
       "      <td>0.207913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v0</th>\n",
       "      <td>1557.0</td>\n",
       "      <td>0.999301</td>\n",
       "      <td>0.003524</td>\n",
       "      <td>0.946393</td>\n",
       "      <td>0.999680</td>\n",
       "      <td>0.999839</td>\n",
       "      <td>0.999926</td>\n",
       "      <td>0.999993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x1</th>\n",
       "      <td>1557.0</td>\n",
       "      <td>0.259424</td>\n",
       "      <td>0.039268</td>\n",
       "      <td>0.145300</td>\n",
       "      <td>0.236737</td>\n",
       "      <td>0.263169</td>\n",
       "      <td>0.282650</td>\n",
       "      <td>0.468079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v31</th>\n",
       "      <td>1557.0</td>\n",
       "      <td>0.975100</td>\n",
       "      <td>0.018599</td>\n",
       "      <td>0.878272</td>\n",
       "      <td>0.969516</td>\n",
       "      <td>0.979351</td>\n",
       "      <td>0.987397</td>\n",
       "      <td>0.998316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x32</th>\n",
       "      <td>1557.0</td>\n",
       "      <td>0.659387</td>\n",
       "      <td>0.087442</td>\n",
       "      <td>0.232325</td>\n",
       "      <td>0.623651</td>\n",
       "      <td>0.671830</td>\n",
       "      <td>0.716763</td>\n",
       "      <td>0.974245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y32</th>\n",
       "      <td>1557.0</td>\n",
       "      <td>0.476513</td>\n",
       "      <td>0.196191</td>\n",
       "      <td>-0.077391</td>\n",
       "      <td>0.331915</td>\n",
       "      <td>0.467582</td>\n",
       "      <td>0.584179</td>\n",
       "      <td>1.027102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>z32</th>\n",
       "      <td>1557.0</td>\n",
       "      <td>0.366132</td>\n",
       "      <td>0.228469</td>\n",
       "      <td>-0.770606</td>\n",
       "      <td>0.227527</td>\n",
       "      <td>0.380349</td>\n",
       "      <td>0.519203</td>\n",
       "      <td>0.882853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v32</th>\n",
       "      <td>1557.0</td>\n",
       "      <td>0.881808</td>\n",
       "      <td>0.082349</td>\n",
       "      <td>0.556156</td>\n",
       "      <td>0.837766</td>\n",
       "      <td>0.891966</td>\n",
       "      <td>0.945091</td>\n",
       "      <td>0.996183</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      count      mean       std       min       25%       50%       75%  \\\n",
       "x0   1557.0  0.270993  0.037887  0.160436  0.248814  0.274044  0.293484   \n",
       "y0   1557.0  0.432748  0.147763 -0.112364  0.330139  0.455499  0.542123   \n",
       "z0   1557.0 -0.201858  0.156447 -1.084517 -0.301598 -0.203921 -0.084499   \n",
       "v0   1557.0  0.999301  0.003524  0.946393  0.999680  0.999839  0.999926   \n",
       "x1   1557.0  0.259424  0.039268  0.145300  0.236737  0.263169  0.282650   \n",
       "..      ...       ...       ...       ...       ...       ...       ...   \n",
       "v31  1557.0  0.975100  0.018599  0.878272  0.969516  0.979351  0.987397   \n",
       "x32  1557.0  0.659387  0.087442  0.232325  0.623651  0.671830  0.716763   \n",
       "y32  1557.0  0.476513  0.196191 -0.077391  0.331915  0.467582  0.584179   \n",
       "z32  1557.0  0.366132  0.228469 -0.770606  0.227527  0.380349  0.519203   \n",
       "v32  1557.0  0.881808  0.082349  0.556156  0.837766  0.891966  0.945091   \n",
       "\n",
       "          max  \n",
       "x0   0.474454  \n",
       "y0   0.862946  \n",
       "z0   0.207913  \n",
       "v0   0.999993  \n",
       "x1   0.468079  \n",
       "..        ...  \n",
       "v31  0.998316  \n",
       "x32  0.974245  \n",
       "y32  1.027102  \n",
       "z32  0.882853  \n",
       "v32  0.996183  \n",
       "\n",
       "[132 rows x 8 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('pose_landmarks.csv')\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>726</th>\n",
       "      <th>727</th>\n",
       "      <th>728</th>\n",
       "      <th>729</th>\n",
       "      <th>730</th>\n",
       "      <th>731</th>\n",
       "      <th>732</th>\n",
       "      <th>733</th>\n",
       "      <th>734</th>\n",
       "      <th>735</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>class_label</th>\n",
       "      <td>Kicking</td>\n",
       "      <td>Kicking</td>\n",
       "      <td>Kicking</td>\n",
       "      <td>Kicking</td>\n",
       "      <td>Kicking</td>\n",
       "      <td>Kicking</td>\n",
       "      <td>Kicking</td>\n",
       "      <td>Kicking</td>\n",
       "      <td>Kicking</td>\n",
       "      <td>Kicking</td>\n",
       "      <td>...</td>\n",
       "      <td>Kicking</td>\n",
       "      <td>Kicking</td>\n",
       "      <td>Kicking</td>\n",
       "      <td>Kicking</td>\n",
       "      <td>Kicking</td>\n",
       "      <td>Kicking</td>\n",
       "      <td>Kicking</td>\n",
       "      <td>Kicking</td>\n",
       "      <td>Kicking</td>\n",
       "      <td>Kicking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x0</th>\n",
       "      <td>0.303104</td>\n",
       "      <td>0.332366</td>\n",
       "      <td>0.341824</td>\n",
       "      <td>0.336903</td>\n",
       "      <td>0.342331</td>\n",
       "      <td>0.33973</td>\n",
       "      <td>0.313527</td>\n",
       "      <td>0.318151</td>\n",
       "      <td>0.316202</td>\n",
       "      <td>0.316802</td>\n",
       "      <td>...</td>\n",
       "      <td>0.270538</td>\n",
       "      <td>0.264021</td>\n",
       "      <td>0.316085</td>\n",
       "      <td>0.271231</td>\n",
       "      <td>0.265952</td>\n",
       "      <td>0.263943</td>\n",
       "      <td>0.264104</td>\n",
       "      <td>0.264316</td>\n",
       "      <td>0.264757</td>\n",
       "      <td>0.317434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y0</th>\n",
       "      <td>0.329654</td>\n",
       "      <td>0.329657</td>\n",
       "      <td>0.317274</td>\n",
       "      <td>0.3075</td>\n",
       "      <td>0.281803</td>\n",
       "      <td>0.26157</td>\n",
       "      <td>0.304855</td>\n",
       "      <td>0.256063</td>\n",
       "      <td>0.250517</td>\n",
       "      <td>0.255854</td>\n",
       "      <td>...</td>\n",
       "      <td>0.574783</td>\n",
       "      <td>0.558444</td>\n",
       "      <td>0.596108</td>\n",
       "      <td>0.542123</td>\n",
       "      <td>0.520092</td>\n",
       "      <td>0.499933</td>\n",
       "      <td>0.490747</td>\n",
       "      <td>0.48139</td>\n",
       "      <td>0.478819</td>\n",
       "      <td>0.599363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>z0</th>\n",
       "      <td>-0.425171</td>\n",
       "      <td>-0.387996</td>\n",
       "      <td>-0.464608</td>\n",
       "      <td>-0.547959</td>\n",
       "      <td>-0.529404</td>\n",
       "      <td>-0.513624</td>\n",
       "      <td>-0.378542</td>\n",
       "      <td>-0.500729</td>\n",
       "      <td>-0.499527</td>\n",
       "      <td>-0.352072</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.265361</td>\n",
       "      <td>-0.340383</td>\n",
       "      <td>-0.111857</td>\n",
       "      <td>-0.290745</td>\n",
       "      <td>-0.310434</td>\n",
       "      <td>-0.316804</td>\n",
       "      <td>-0.297326</td>\n",
       "      <td>-0.29469</td>\n",
       "      <td>-0.253086</td>\n",
       "      <td>-0.087718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v0</th>\n",
       "      <td>0.99991</td>\n",
       "      <td>0.999917</td>\n",
       "      <td>0.99992</td>\n",
       "      <td>0.999879</td>\n",
       "      <td>0.999885</td>\n",
       "      <td>0.999894</td>\n",
       "      <td>0.999904</td>\n",
       "      <td>0.999913</td>\n",
       "      <td>0.99992</td>\n",
       "      <td>0.999922</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999346</td>\n",
       "      <td>0.999396</td>\n",
       "      <td>0.999455</td>\n",
       "      <td>0.999496</td>\n",
       "      <td>0.999539</td>\n",
       "      <td>0.999582</td>\n",
       "      <td>0.999621</td>\n",
       "      <td>0.999656</td>\n",
       "      <td>0.999678</td>\n",
       "      <td>0.999709</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 736 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0         1         2         3         4         5    \\\n",
       "class_label   Kicking   Kicking   Kicking   Kicking   Kicking   Kicking   \n",
       "x0           0.303104  0.332366  0.341824  0.336903  0.342331   0.33973   \n",
       "y0           0.329654  0.329657  0.317274    0.3075  0.281803   0.26157   \n",
       "z0          -0.425171 -0.387996 -0.464608 -0.547959 -0.529404 -0.513624   \n",
       "v0            0.99991  0.999917   0.99992  0.999879  0.999885  0.999894   \n",
       "\n",
       "                  6         7         8         9    ...       726       727  \\\n",
       "class_label   Kicking   Kicking   Kicking   Kicking  ...   Kicking   Kicking   \n",
       "x0           0.313527  0.318151  0.316202  0.316802  ...  0.270538  0.264021   \n",
       "y0           0.304855  0.256063  0.250517  0.255854  ...  0.574783  0.558444   \n",
       "z0          -0.378542 -0.500729 -0.499527 -0.352072  ... -0.265361 -0.340383   \n",
       "v0           0.999904  0.999913   0.99992  0.999922  ...  0.999346  0.999396   \n",
       "\n",
       "                  728       729       730       731       732       733  \\\n",
       "class_label   Kicking   Kicking   Kicking   Kicking   Kicking   Kicking   \n",
       "x0           0.316085  0.271231  0.265952  0.263943  0.264104  0.264316   \n",
       "y0           0.596108  0.542123  0.520092  0.499933  0.490747   0.48139   \n",
       "z0          -0.111857 -0.290745 -0.310434 -0.316804 -0.297326  -0.29469   \n",
       "v0           0.999455  0.999496  0.999539  0.999582  0.999621  0.999656   \n",
       "\n",
       "                  734       735  \n",
       "class_label   Kicking   Kicking  \n",
       "x0           0.264757  0.317434  \n",
       "y0           0.478819  0.599363  \n",
       "z0          -0.253086 -0.087718  \n",
       "v0           0.999678  0.999709  \n",
       "\n",
       "[5 rows x 736 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['class_label']=='Kicking'].T.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shapes: (1245, 132) (312, 132)\n",
      "Label distribution (train): 0    588\n",
      "2    585\n",
      "1     72\n",
      "Name: count, dtype: int64\n",
      "Label distribution (test): 0    148\n",
      "2    146\n",
      "1     18\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('pose_landmarks.csv')\n",
    "\n",
    "# Separate out the label\n",
    "y_str = df['class_label'].values  # e.g. \"Kick\", \"Punching\", \"Non-violent\"\n",
    "\n",
    "# Convert to numeric 0, 1, 2\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y_str) \n",
    "# Suppose: \"Kick\" -> 0, \"Punching\" -> 2, \"Non-violent\" -> 1 (the exact mapping depends on alphabetical order or input)\n",
    "\n",
    "X = df.drop('class_label', axis=1).values  # shape: (num_frames, 132)\n",
    "\n",
    "# Optional: scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, \n",
    "    y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y  # keep class balance\n",
    ")\n",
    "\n",
    "print(\"Data shapes:\", X_train.shape, X_test.shape)\n",
    "print(\"Label distribution (train):\", pd.Series(y_train).value_counts())\n",
    "print(\"Label distribution (test):\", pd.Series(y_test).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train a Classifier on Pose CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Train Machine Learning Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = {\n",
    "    'lr':make_pipeline(StandardScaler(), LogisticRegression()),\n",
    "    'rc':make_pipeline(StandardScaler(), RidgeClassifier()),\n",
    "    'rf':make_pipeline(StandardScaler(), RandomForestClassifier()),\n",
    "    'gb':make_pipeline(StandardScaler(), GradientBoostingClassifier()),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_models = {}\n",
    "for algo, pipeline in pipelines.items():\n",
    "    model = pipeline.fit(X_train, y_train)\n",
    "    fit_models[algo] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                 ('logisticregression', LogisticRegression())]),\n",
       " 'rc': Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                 ('ridgeclassifier', RidgeClassifier())]),\n",
       " 'rf': Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                 ('randomforestclassifier', RandomForestClassifier())]),\n",
       " 'gb': Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                 ('gradientboostingclassifier', GradientBoostingClassifier())])}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 0, 2, 0, 0, 2, 0, 0, 2, 2, 0, 0, 2, 2, 0, 0, 0, 1, 2, 0, 0,\n",
       "       2, 0, 2, 2, 0, 2, 2, 2, 2, 0, 2, 0, 0, 2, 0, 2, 2, 0, 2, 0, 0, 2,\n",
       "       1, 2, 0, 2, 0, 0, 0, 2, 2, 2, 2, 2, 0, 2, 0, 2, 2, 1, 2, 2, 0, 0,\n",
       "       2, 0, 0, 0, 2, 2, 1, 0, 0, 2, 1, 0, 0, 0, 2, 2, 2, 0, 0, 2, 2, 0,\n",
       "       0, 0, 2, 2, 0, 0, 2, 2, 2, 2, 0, 2, 2, 2, 0, 2, 0, 2, 0, 0, 2, 0,\n",
       "       0, 0, 1, 0, 2, 2, 2, 2, 1, 2, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 2, 0,\n",
       "       0, 2, 0, 0, 0, 2, 0, 2, 0, 1, 2, 2, 0, 0, 2, 2, 0, 0, 0, 2, 0, 0,\n",
       "       0, 2, 0, 0, 2, 2, 0, 2, 1, 2, 0, 2, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 0, 0, 2, 0, 0, 0, 2, 2, 2, 0, 1, 2, 2, 0, 2, 0, 2, 0, 2, 0, 0,\n",
       "       0, 2, 1, 1, 2, 2, 2, 0, 0, 2, 2, 2, 0, 2, 2, 2, 0, 2, 0, 2, 0, 2,\n",
       "       2, 0, 2, 2, 2, 0, 0, 1, 2, 0, 2, 2, 2, 0, 0, 2, 0, 0, 0, 2, 0, 1,\n",
       "       0, 2, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 2,\n",
       "       0, 2, 0, 2, 0, 0, 2, 2, 2, 2, 2, 0, 2, 0, 0, 0, 0, 1, 0, 0, 2, 2,\n",
       "       2, 2, 0, 2, 0, 2, 2, 0, 0, 0, 2, 2, 0, 0, 2, 0, 2, 2, 0, 0, 2, 0,\n",
       "       2, 2, 0, 2])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_models['rc'].predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Train TensorFlow Model (Multi-Class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.1 DNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,512</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,512\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │            \u001b[38;5;34m99\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,691</span> (41.76 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m10,691\u001b[0m (41.76 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,691</span> (41.76 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m10,691\u001b[0m (41.76 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5153 - loss: 1.0754 - val_accuracy: 0.8032 - val_loss: 0.4377\n",
      "Epoch 2/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7946 - loss: 0.5088 - val_accuracy: 0.9116 - val_loss: 0.2931\n",
      "Epoch 3/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8313 - loss: 0.4316 - val_accuracy: 0.9317 - val_loss: 0.2458\n",
      "Epoch 4/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8757 - loss: 0.3232 - val_accuracy: 0.9438 - val_loss: 0.2007\n",
      "Epoch 5/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8930 - loss: 0.2807 - val_accuracy: 0.9438 - val_loss: 0.1793\n",
      "Epoch 6/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8937 - loss: 0.2649 - val_accuracy: 0.9518 - val_loss: 0.1470\n",
      "Epoch 7/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9315 - loss: 0.1911 - val_accuracy: 0.9518 - val_loss: 0.1299\n",
      "Epoch 8/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9218 - loss: 0.1977 - val_accuracy: 0.9558 - val_loss: 0.1131\n",
      "Epoch 9/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9370 - loss: 0.1617 - val_accuracy: 0.9679 - val_loss: 0.0979\n",
      "Epoch 10/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9420 - loss: 0.1507 - val_accuracy: 0.9598 - val_loss: 0.0961\n",
      "Epoch 11/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9659 - loss: 0.1068 - val_accuracy: 0.9679 - val_loss: 0.0884\n",
      "Epoch 12/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9557 - loss: 0.1237 - val_accuracy: 0.9799 - val_loss: 0.0707\n",
      "Epoch 13/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9604 - loss: 0.1099 - val_accuracy: 0.9920 - val_loss: 0.0615\n",
      "Epoch 14/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9593 - loss: 0.1116 - val_accuracy: 0.9799 - val_loss: 0.0626\n",
      "Epoch 15/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9682 - loss: 0.0779 - val_accuracy: 0.9799 - val_loss: 0.0626\n",
      "Epoch 16/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9742 - loss: 0.0776 - val_accuracy: 0.9880 - val_loss: 0.0507\n",
      "Epoch 17/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9738 - loss: 0.0709 - val_accuracy: 0.9839 - val_loss: 0.0466\n",
      "Epoch 18/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9800 - loss: 0.0604 - val_accuracy: 0.9920 - val_loss: 0.0452\n",
      "Epoch 19/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9729 - loss: 0.0712 - val_accuracy: 0.9839 - val_loss: 0.0448\n",
      "Epoch 20/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9806 - loss: 0.0617 - val_accuracy: 0.9920 - val_loss: 0.0378\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "num_classes = len(label_encoder.classes_)  # should be 3\n",
    "\n",
    "lr_model = tf.keras.Sequential([\n",
    "    layers.Input(shape=(X_train.shape[1],)),  # 132\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(num_classes, activation='softmax')  # for 3-class classification\n",
    "])\n",
    "\n",
    "lr_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "lr_model.summary()\n",
    "\n",
    "history = lr_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=20,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9800 - loss: 0.0509 \n",
      "Test Accuracy: 98.40%\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "Confusion Matrix:\n",
      " [[145   0   3]\n",
      " [  0  18   0]\n",
      " [  2   0 144]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Kicking       0.99      0.98      0.98       148\n",
      " Non-violent       1.00      1.00      1.00        18\n",
      "    Punching       0.98      0.99      0.98       146\n",
      "\n",
      "    accuracy                           0.98       312\n",
      "   macro avg       0.99      0.99      0.99       312\n",
      "weighted avg       0.98      0.98      0.98       312\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = lr_model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
    "\n",
    "# Optional confusion matrix\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "y_pred = np.argmax(lr_model.predict(X_test), axis=1)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "# Show detailed metrics\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "with open('lr_behaviour.pkl', 'wb') as f:\n",
    "    pickle.dump(fit_models['rf'], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2. LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "num_classes = len(label_encoder.classes_)  # should be 3\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Input(shape=(X_train.shape[1],)),  # 132\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(num_classes, activation='softmax')  # for 3-class classification\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=20,\n",
    "    batch_size=32\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Make Detection with Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lr_behaviour.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Mediapipe Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv.VideoCapture(0)\n",
    "# Initiate holistic model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # Recolor Feed\n",
    "        image = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False        \n",
    "        \n",
    "        # Make Detections\n",
    "        results = holistic.process(image)\n",
    "        # print(results.face_landmarks)\n",
    "        \n",
    "        # face_landmarks, pose_landmarks, left_hand_landmarks, right_hand_landmarks\n",
    "        \n",
    "        # Recolor image back to BGR for rendering\n",
    "        image.flags.writeable = True   \n",
    "        image = cv.cvtColor(image, cv.COLOR_RGB2BGR)\n",
    "        \n",
    "        # 4. Pose Detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "        # Export coordinates\n",
    "        try:\n",
    "            # Extract Pose landmarks\n",
    "            pose = results.pose_landmarks.landmark\n",
    "            pose_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in pose]).flatten())\n",
    "            \n",
    "            # Extract Face landmarks\n",
    "            face = results.face_landmarks.landmark\n",
    "            face_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in face]).flatten())\n",
    "            \n",
    "            # Concate rows\n",
    "            row = pose_row+face_row\n",
    "            \n",
    "#             # Append class name \n",
    "#             row.insert(0, class_name)\n",
    "            \n",
    "#             # Export to CSV\n",
    "#             with open('coords.csv', mode='a', newline='') as f:\n",
    "#                 csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "#                 csv_writer.writerow(row) \n",
    "\n",
    "            # Make Detections\n",
    "            X = pd.DataFrame([row])\n",
    "            body_language_class = model.predict(X)[0]\n",
    "            body_language_prob = model.predict_proba(X)[0]\n",
    "            print(body_language_class, body_language_prob)\n",
    "            \n",
    "            # Grab ear coords\n",
    "            coords = tuple(np.multiply(\n",
    "                            np.array(\n",
    "                                (results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_EAR].x, \n",
    "                                 results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_EAR].y))\n",
    "                        , [640,480]).astype(int))\n",
    "            \n",
    "            cv.rectangle(image, \n",
    "                          (coords[0], coords[1]+5), \n",
    "                          (coords[0]+len(body_language_class)*20, coords[1]-30), \n",
    "                          (245, 117, 16), -1)\n",
    "            cv.putText(image, body_language_class, coords, \n",
    "                        cv.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv.LINE_AA)\n",
    "            \n",
    "            # Get status box\n",
    "            cv.rectangle(image, (0,0), (250, 60), (245, 117, 16), -1)\n",
    "            \n",
    "            # Display Class\n",
    "            cv.putText(image, 'CLASS'\n",
    "                        , (95,12), cv.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv.LINE_AA)\n",
    "            cv.putText(image, body_language_class.split(' ')[0]\n",
    "                        , (90,40), cv.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv.LINE_AA)\n",
    "            \n",
    "            # Display Probability\n",
    "            cv.putText(image, 'PROB'\n",
    "                        , (15,12), cv.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv.LINE_AA)\n",
    "            cv.putText(image, str(round(body_language_prob[np.argmax(body_language_prob)],2))\n",
    "                        , (10,40), cv.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv.LINE_AA)\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "                        \n",
    "        cv.imshow('Raw Webcam Feed', image)\n",
    "\n",
    "        if cv.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediapipe_pose",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
