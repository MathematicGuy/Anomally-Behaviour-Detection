{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading a Video and Saving Frames in Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes: 3 subfolders within 'Single_person_violent' => Kicking, Punching, Non-violent\n",
    "CLASSES = [\"Kicking\", \"Punching\", \"Standing\"]\n",
    "\n",
    "# We will define a fixed sequence length to handle variable-length videos\n",
    "MAX_SEQ_LEN = 5 # example: 30 frames per video\n",
    "\n",
    "# Set how many frames to skip when reading a video (to reduce computational load)\n",
    "SKIP_RATE = 1  # example: capture 1 frame out of every 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Extract Pose Sequence from a Single Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function opens a video file with OpenCV (cv.VideoCapture).\n",
    "\n",
    "It skips frames by advancing the capture index so we only process 1 out of every SKIP_RATE frames.\n",
    "\n",
    "For each processed frame, it uses MediaPipe Pose to detect 33 landmarks.\n",
    "\n",
    "Each landmark has 4 values: (x, y, z, visibility), so for 33 landmarks, we get 132 values in one frame.\n",
    "\n",
    "If no pose is detected, we append a zero vector (132 zeros).\n",
    "\n",
    "Finally, it returns a NumPy array of shape (T, 132), where T is the number of frames we processed in the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "def get_pose_sequence_from_video(video_path, skip_rate=5):\n",
    "    \"\"\"\n",
    "    Opens a video, reads frames at a specified skip_rate,\n",
    "    and returns a NumPy array of shape (num_frames, 132)\n",
    "    containing (x, y, z, visibility) for 33 pose landmarks.\n",
    "    \"\"\"\n",
    "    cap = cv.VideoCapture(video_path)\n",
    "    \n",
    "    # List to store the per-frame keypoints\n",
    "    sequence = []\n",
    "    \n",
    "    with mp_pose.Pose(min_detection_confidence=0.5, \n",
    "                      min_tracking_confidence=0.5) as pose_model:\n",
    "        \n",
    "        frame_index = 0  # keep track of frame index\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break  # no more frames\n",
    "            \n",
    "            # Process only 1 out of every 'skip_rate' frames\n",
    "            if frame_index % skip_rate == 0:\n",
    "                # Convert BGR to RGB\n",
    "                rgb_frame = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "                results = pose_model.process(rgb_frame)\n",
    "\t\t\t\t\n",
    "                if results.pose_landmarks:\n",
    "                    # Flatten 33 landmarks × 4 = 132 values\n",
    "                    keypoints = []\n",
    "                    for lm in results.pose_landmarks.landmark:\n",
    "                        keypoints.extend([lm.x, lm.y, lm.z, lm.visibility])\n",
    "                    sequence.append(keypoints)\n",
    "                else:\n",
    "                    # If no pose was detected, append a zero vector of length 132\n",
    "                    sequence.append([0]*132)\n",
    "            \n",
    "            frame_index += 1  # increment frame counter\n",
    "    \n",
    "    cap.release()\n",
    "    return np.array(sequence)  # shape (num_frames, 132)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 75 sequences.\n"
     ]
    }
   ],
   "source": [
    "data = np.load(\"extracted_data/pose_dataset_3.npz\", allow_pickle=True)\n",
    "X_sequences = list(data[\"X\"])\n",
    "y_labels = list(data[\"y\"])\n",
    "\n",
    "print(f\"✅ Loaded {len(X_sequences)} sequences.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pad or Truncate Sequences\n",
    "LSTM/GRU networks expect uniform sequence lengths in a batch.\n",
    "\n",
    "We define a function pad_or_truncate_sequence that ensures each sequence has exactly MAX_SEQ_LEN frames.\n",
    "\n",
    "+ If a sequence is longer than MAX_SEQ_LEN, we take the first MAX_SEQ_LEN frames.\n",
    "\n",
    "+ If it’s shorter, we pad with zeros at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_or_truncate_sequence(seq, max_len=30):\n",
    "    \"\"\"\n",
    "    seq: (T, 132) array for T frames\n",
    "    Returns an array of shape (max_len, 132).\n",
    "    \"\"\"\n",
    "    \n",
    "    length = seq.shape[0]\n",
    "    num_features = seq.shape[1]\n",
    "    \n",
    "    # print(length, num_features)\n",
    "    \n",
    "    if length > max_len:\n",
    "        # Truncate\n",
    "        return seq[:max_len, :]\n",
    "    else:\n",
    "        # Pad with zeros\n",
    "        padded = np.zeros((max_len, num_features))\n",
    "        padded[:length, :] = seq\n",
    "        return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence Length Statistics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence_Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>75.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>75.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>79.725260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>33.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>47.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>79.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>338.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sequence_Length\n",
       "count        75.000000\n",
       "mean         75.533333\n",
       "std          79.725260\n",
       "min           1.000000\n",
       "25%          33.000000\n",
       "50%          47.000000\n",
       "75%          79.500000\n",
       "max         338.000000"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate average sequence length from X_sequences\n",
    "seq_lengths = [seq.shape[0] for seq in X_sequences]\n",
    "# Convert sequence lengths to a DataFrame\n",
    "df_seq_lengths = pd.DataFrame(seq_lengths, columns=['Sequence_Length'])\n",
    "\n",
    "# Get statistical description\n",
    "print(\"Sequence Length Statistics:\")\n",
    "df_seq_lengths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final shape of data: (75, 5, 132)\n"
     ]
    }
   ],
   "source": [
    "X_seq_padded = []\n",
    "\n",
    "for seq in X_sequences:\n",
    "    seq_padded = pad_or_truncate_sequence(seq, max_len=MAX_SEQ_LEN)\n",
    "    X_seq_padded.append(seq_padded)\n",
    "\n",
    "X_seq_padded = np.array(X_seq_padded)  # shape => (num_videos, MAX_SEQ_LEN, 132)\n",
    "\n",
    "print(\"Final shape of data:\", X_seq_padded.shape)\n",
    "# Should be (N, 20, 132) if MAX_SEQ_LEN=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write to CSV file for visualization and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Encode Labels and Split Data\n",
    "\n",
    "We use LabelEncoder to convert \"Kicking\", \"Punching\", \"Non-violent\" into numeric IDs: e.g. 0, 1, 2.\n",
    "\n",
    "Then we split into train/test sets (e.g., 80/20) for fair evaluation.\n",
    "\n",
    "We store them as X_train, X_test, y_train, y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (60, 5, 132) Test shape: (15, 5, 132)\n",
      "Train labels shape: (60,) Test labels shape: (15,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_int = label_encoder.fit_transform(y_labels)  \n",
    "# e.g. \"Kick\"->0, \"Punching\"->2, \"Standing\"->1 (the mapping depends on alphabetical order)\n",
    "\n",
    "# Convert to NumPy\n",
    "y_int = np.array(y_int)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_seq_padded, \n",
    "    y_int, \n",
    "    test_size=0.2, \n",
    "    stratify=y_int,  # keep classes balanced\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n",
    "print(\"Train labels shape:\", y_train.shape, \"Test labels shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of videos in the 'Single_person_violent/Kicking' folder: 20\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "BASE_DIR = \"Single_person_violent\"\n",
    "KICKING_DIR = os.path.join(BASE_DIR, \"Punching\")\n",
    "\n",
    "# Use glob to list all .mp4 or .avi files in the Kicking directory\n",
    "video_paths = glob.glob(os.path.join(KICKING_DIR, \"*.mp4\")) + glob.glob(os.path.join(KICKING_DIR, \"*.avi\"))\n",
    "\n",
    "num_videos = len(video_paths)\n",
    "print(f\"Total number of videos in the 'Single_person_violent/Kicking' folder: {num_videos}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Make Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N: 75 \n",
      "MAX_SEQ_LEN: 5 \n",
      "NUM_FEATURES: 132 \n",
      "num_classes: 3\n"
     ]
    }
   ],
   "source": [
    "N, MAX_SEQ_LEN, NUM_FEATURES = X_seq_padded.shape\n",
    "NUM_CLASSES = len(np.unique(y_int))\n",
    "\n",
    "print(\"N:\", N, \"\\nMAX_SEQ_LEN:\", MAX_SEQ_LEN, \"\\nNUM_FEATURES:\", NUM_FEATURES, \"\\nnum_classes:\", NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load infer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQ_LEN = 5\n",
    "NUM_FEATURES = 132  # 33 pose landmarks × 4 (x, y, z, visibility)\n",
    "CLASSES = [\"Standing\", \"Kicking\", \"Punching\"]\n",
    "infer_mode = \"dnn\"  # Choose: \"dnn\", \"lstm\", \"gru\"\n",
    "tflite_path = \"utils/weight_5seq/dnn_model.tflite\"  # Update if using another model\n",
    "\n",
    "path = \"utils/weight_5seq/\"\n",
    "lstm_path = path + \"lstm_model.h5\"\n",
    "gru_path = path + \"gru_model.h5\"\n",
    "dnn_pth = path + \"dnn_model.h5\"\n",
    "\n",
    "#Load model\n",
    "lstm_infer_model = tf.keras.models.load_model(lstm_path)\n",
    "gru_infer_model = tf.keras.models.load_model(gru_path)\n",
    "dnn_infer_model = tf.keras.models.load_model(dnn_pth)\n",
    "infer_model = dnn_infer_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert model to TFLITE to improve inference speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\boboi\\AppData\\Local\\Temp\\tmp3_ka22vc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\boboi\\AppData\\Local\\Temp\\tmp3_ka22vc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\boboi\\AppData\\Local\\Temp\\tmp3_ka22vc'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 660), dtype=tf.float32, name='input_layer_13')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 3), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2597754233328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2597754032320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2597805798880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2597805787616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2597753152688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2597753157440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Convert the model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(infer_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]  # Optional optimization\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFLite model saved to: utils/weight_5seq/dnn_model.tflite\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.makedirs(\"utils/weight_5seq\", exist_ok=True)\n",
    "\n",
    "# Define save path\n",
    "tflite_path = \"utils/weight_5seq/dnn_model.tflite\"\n",
    "\n",
    "# Write to disk\n",
    "with open(tflite_path, \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"TFLite model saved to:\", tflite_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load TFLite Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CODE\\IDE\\Anaconda\\envs\\mediapipe_pose\\lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# Buffer to store latest MAX_SEQ_LEN frames of keypoints\n",
    "buffer = collections.deque(maxlen=MAX_SEQ_LEN)\n",
    "\n",
    "# Start webcam\n",
    "cap = cv.VideoCapture(0)\n",
    "cap.set(cv.CAP_PROP_FRAME_WIDTH, 960)\n",
    "cap.set(cv.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Resize frame to consistent shape (optional for clarity)\n",
    "    frame = cv.resize(frame, (960, 720))\n",
    "\n",
    "    # Convert to RGB and get pose\n",
    "    rgb = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "    results = pose.process(rgb)\n",
    "\n",
    "    # Extract pose keypoints\n",
    "    if results.pose_landmarks:\n",
    "        keypoints = []\n",
    "        for lm in results.pose_landmarks.landmark:\n",
    "            keypoints.extend([lm.x, lm.y, lm.z, lm.visibility])\n",
    "    else:\n",
    "        keypoints = [0] * NUM_FEATURES\n",
    "\n",
    "    # Add keypoints to buffer\n",
    "    buffer.append(keypoints)\n",
    "\n",
    "    if len(buffer) == MAX_SEQ_LEN:\n",
    "        seq = np.array(buffer)\n",
    "\n",
    "        if infer_model == dnn_infer_model:\n",
    "            seq_input = seq.reshape(1, -1)  # DNN input shape: (1, 660)\n",
    "        else:\n",
    "            seq_input = np.expand_dims(seq, axis=0)  # LSTM/GRU input shape: (1, 5, 132)\n",
    "\n",
    "        pred = infer_model.predict(seq_input, verbose=0)\n",
    "        class_id = np.argmax(pred)\n",
    "        confidence = float(pred[0][class_id]) * 100\n",
    "        class_name = CLASSES[class_id]\n",
    "        \n",
    "        if class_name == \"Standing\":\n",
    "            display_text = f\"{class_name} ({confidence:.1f}%), Non-Violent\"\n",
    "        else:\n",
    "            display_text = f\"{class_name} ({confidence:.1f}%), Violent\"\n",
    "            \n",
    "        # Show prediction on frame\n",
    "        cv.putText(frame, f'Action: {display_text}', (30, 50),\n",
    "                   cv.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 3)\n",
    "    else:\n",
    "        display_text = \"Non-Violent\"\n",
    "        cv.putText(frame, f'Action: {display_text}', (30, 50),\n",
    "                   cv.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 3)\n",
    "\n",
    "    # Draw pose landmarks\n",
    "    if results.pose_landmarks:\n",
    "        mp.solutions.drawing_utils.draw_landmarks(\n",
    "            frame,\n",
    "            results.pose_landmarks,\n",
    "            mp_pose.POSE_CONNECTIONS,\n",
    "            landmark_drawing_spec=mp.solutions.drawing_utils.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=2),\n",
    "            connection_drawing_spec=mp.solutions.drawing_utils.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2)\n",
    "        )\n",
    "\n",
    "    # Display the frame\n",
    "    cv.imshow(\"Live Action Detection\", frame)\n",
    "\n",
    "    # Press 'q' to exit\n",
    "    if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Cleanup\n",
    "cap.release()\n",
    "cv.destroyAllWindows()\n",
    "pose.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on TFLite "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# Buffer to store latest MAX_SEQ_LEN frames of keypoints\n",
    "buffer = collections.deque(maxlen=MAX_SEQ_LEN)\n",
    "\n",
    "# Start webcam\n",
    "cap = cv.VideoCapture(0)\n",
    "cap.set(cv.CAP_PROP_FRAME_WIDTH, 640)\n",
    "cap.set(cv.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "# FPS calculation variables\n",
    "prev_frame_time = 0\n",
    "fps = 0\n",
    "\n",
    "while True:\n",
    "    start = cv.getTickCount()\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv.resize(frame, (960, 720))\n",
    "    rgb = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "    results = pose.process(rgb)\n",
    "\n",
    "    #  Extract Keypoints \n",
    "    if results.pose_landmarks:\n",
    "        keypoints = []\n",
    "        for lm in results.pose_landmarks.landmark:\n",
    "            keypoints.extend([lm.x, lm.y, lm.z, lm.visibility])\n",
    "    else:\n",
    "        keypoints = [0] * NUM_FEATURES  # Fill with zeros if no pose\n",
    "\n",
    "    buffer.append(keypoints)\n",
    "\n",
    "    #  Run Inference if Buffer is Full \n",
    "    if len(buffer) == MAX_SEQ_LEN:\n",
    "        seq = np.array(buffer)\n",
    "\n",
    "        # Reshape input for DNN or LSTM/GRU\n",
    "        if infer_mode == \"dnn\":\n",
    "            seq_input = seq.reshape(1, -1)  # (1, 660)\n",
    "        else:\n",
    "            seq_input = np.expand_dims(seq, axis=0)  # (1, 5, 132)\n",
    "\n",
    "        # Set input & invoke TFLite\n",
    "        interpreter.set_tensor(input_details[0]['index'], seq_input.astype(np.float32))\n",
    "        interpreter.invoke()\n",
    "        pred = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "        # Post-process prediction\n",
    "        class_id = np.argmax(pred)\n",
    "        confidence = float(pred[0][class_id]) * 100\n",
    "        class_name = CLASSES[class_id]\n",
    "\n",
    "        if class_name == \"Standing\":\n",
    "            display_text = f\"{class_name} ({confidence:.1f}%), Non-Violent\"\n",
    "        else:\n",
    "            display_text = f\"{class_name} ({confidence:.1f}%), Violent\"\n",
    "    else:\n",
    "        display_text = \"Non-Violent\"\n",
    "\n",
    "    #  Draw Pose Landmarks \n",
    "    if results.pose_landmarks:\n",
    "        mp.solutions.drawing_utils.draw_landmarks(\n",
    "            frame,\n",
    "            results.pose_landmarks,\n",
    "            mp_pose.POSE_CONNECTIONS,\n",
    "            landmark_drawing_spec=mp.solutions.drawing_utils.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=2),\n",
    "            connection_drawing_spec=mp.solutions.drawing_utils.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2)\n",
    "        )\n",
    "\n",
    "    #  Calculate & Display FPS \n",
    "    end = cv.getTickCount()\n",
    "    elapsed_time = (end - start) / cv.getTickFrequency()\n",
    "    fps = 1 / elapsed_time if elapsed_time > 0 else 0\n",
    "\n",
    "    # === Draw Info on Frame ===\n",
    "    cv.putText(frame, f'Action: {display_text}', (30, 50),\n",
    "               cv.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 3)\n",
    "    cv.putText(frame, f'FPS: {int(fps)}', (30, 90),\n",
    "               cv.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    cv.imshow(\"Live Action Detection\", frame)\n",
    "\n",
    "    # Exit with 'q'\n",
    "    if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Cleanup\n",
    "cap.release()\n",
    "cv.destroyAllWindows()\n",
    "pose.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: kicking23.avi\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import collections\n",
    "import mediapipe as mp\n",
    "\n",
    "# Use the test_video_paths that's already defined in the context\n",
    "test_video_path = \"Single_person_violent\\Kicking\\kicking23.avi\"\n",
    "print(f\"Processing: {os.path.basename(test_video_path)}\")\n",
    "\n",
    "# Define global constants (make sure they're set)\n",
    "# MAX_SEQ_LEN, NUM_FEATURES, CLASSES, infer_model, dnn_infer_model should be defined above\n",
    "\n",
    "# ✅ Function to rotate horizontal frames to vertical\n",
    "# def ensure_vertical_orientation(frame):\n",
    "# \theight, width = frame.shape[:2]\n",
    "# \tif width > height:\n",
    "# \t\t# Rotate 90 degrees counter-clockwise\n",
    "# \t\t# frame = cv.rotate(frame, cv.ROTATE_90_COUNTERCLOCKWISE)\n",
    "# \treturn frame\n",
    "\n",
    "# Initialize MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(\n",
    "\tmin_detection_confidence=0.5,\n",
    "\tmin_tracking_confidence=0.5,\n",
    "\tmodel_complexity=1\n",
    ")\n",
    "\n",
    "# Open video file\n",
    "cap = cv.VideoCapture(test_video_path)\n",
    "\n",
    "# Buffer to store latest MAX_SEQ_LEN frames of keypoints\n",
    "buffer = collections.deque(maxlen=MAX_SEQ_LEN)\n",
    "\n",
    "while True:\n",
    "\tret, frame = cap.read()\n",
    "\tif not ret:\n",
    "\t\tbreak\n",
    "\n",
    "\t# ✅ Ensure the frame is vertically oriented\n",
    "\t# frame = ensure_vertical_orientation(frame)\n",
    "\n",
    "\t# Convert to RGB and get pose\n",
    "\trgb = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "\tresults = pose.process(rgb)\n",
    "\n",
    "\t# Extract pose keypoints\n",
    "\tif results.pose_landmarks:\n",
    "\t\tkeypoints = []\n",
    "\t\tfor lm in results.pose_landmarks.landmark:\n",
    "\t\t\tkeypoints.extend([lm.x, lm.y, lm.z, lm.visibility])\n",
    "\telse:\n",
    "\t\tkeypoints = [0] * NUM_FEATURES\n",
    "\n",
    "\t# Add keypoints to buffer\n",
    "\tbuffer.append(keypoints)\n",
    "\n",
    "\tif len(buffer) == MAX_SEQ_LEN:\n",
    "\t\tseq = np.array(buffer)\n",
    "\n",
    "\t\tif infer_model == dnn_infer_model:\n",
    "\t\t\tseq_input = seq.reshape(1, -1)  # DNN input shape: (1, 660)\n",
    "\t\telse:\n",
    "\t\t\tseq_input = np.expand_dims(seq, axis=0)  # LSTM/GRU input shape: (1, 5, 132)\n",
    "\n",
    "\t\tpred = infer_model.predict(seq_input, verbose=0)\n",
    "\t\tclass_id = np.argmax(pred)\n",
    "\t\tconfidence = float(pred[0][class_id]) * 100\n",
    "\t\tclass_name = CLASSES[class_id]\n",
    "\n",
    "\t\tif class_name == \"Standing\":\n",
    "\t\t\tdisplay_text = f\"{class_name} ({confidence:.1f}%), Non-Violent\"\n",
    "\t\telse:\n",
    "\t\t\tdisplay_text = f\"{class_name} ({confidence:.1f}%), Violent\"\n",
    "\n",
    "\t\t# Show prediction on frame\n",
    "\t\tcv.putText(frame, f'Action: {display_text}', (30, 50),\n",
    "\t\t\t\t   cv.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 3)\n",
    "\n",
    "\t# Draw pose landmarks\n",
    "\tif results.pose_landmarks:\n",
    "\t\tmp.solutions.drawing_utils.draw_landmarks(\n",
    "\t\t\tframe,\n",
    "\t\t\tresults.pose_landmarks,\n",
    "\t\t\tmp_pose.POSE_CONNECTIONS,\n",
    "\t\t\tlandmark_drawing_spec=mp.solutions.drawing_utils.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=2),\n",
    "\t\t\tconnection_drawing_spec=mp.solutions.drawing_utils.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2)\n",
    "\t\t)\n",
    "\n",
    "\t# Display the frame\n",
    "\tcv.imshow(f\"Action Detection - {os.path.basename(test_video_path)}\", frame)\n",
    "\n",
    "\t# Press 'q' to exit\n",
    "\tif cv.waitKey(1) & 0xFF == ord('q'):\n",
    "\t\tbreak\n",
    "\n",
    "# Cleanup\n",
    "cap.release()\n",
    "cv.destroyAllWindows()\n",
    "pose.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediapipe_pose",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
