{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading a Video and Saving Frames in Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes: 3 subfolders within 'Single_person_violent' => Kicking, Punching, Non-violent\n",
    "CLASSES = [\"Kicking\", \"Punching\", \"Standing\"]\n",
    "\n",
    "# We will define a fixed sequence length to handle variable-length videos\n",
    "MAX_SEQ_LEN = 5 # example: 30 frames per video\n",
    "\n",
    "# Set how many frames to skip when reading a video (to reduce computational load)\n",
    "SKIP_RATE = 1  # example: capture 1 frame out of every 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Extract Pose Sequence from a Single Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function opens a video file with OpenCV (cv.VideoCapture).\n",
    "\n",
    "It skips frames by advancing the capture index so we only process 1 out of every SKIP_RATE frames.\n",
    "\n",
    "For each processed frame, it uses MediaPipe Pose to detect 33 landmarks.\n",
    "\n",
    "Each landmark has 4 values: (x, y, z, visibility), so for 33 landmarks, we get 132 values in one frame.\n",
    "\n",
    "If no pose is detected, we append a zero vector (132 zeros).\n",
    "\n",
    "Finally, it returns a NumPy array of shape (T, 132), where T is the number of frames we processed in the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "def get_pose_sequence_from_video(video_path, skip_rate=5):\n",
    "    \"\"\"\n",
    "    Opens a video, reads frames at a specified skip_rate,\n",
    "    and returns a NumPy array of shape (num_frames, 132)\n",
    "    containing (x, y, z, visibility) for 33 pose landmarks.\n",
    "    \"\"\"\n",
    "    cap = cv.VideoCapture(video_path)\n",
    "    \n",
    "    # List to store the per-frame keypoints\n",
    "    sequence = []\n",
    "    \n",
    "    with mp_pose.Pose(min_detection_confidence=0.5, \n",
    "                      min_tracking_confidence=0.5) as pose_model:\n",
    "        \n",
    "        frame_index = 0  # keep track of frame index\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break  # no more frames\n",
    "            \n",
    "            # Process only 1 out of every 'skip_rate' frames\n",
    "            if frame_index % skip_rate == 0:\n",
    "                # Convert BGR to RGB\n",
    "                rgb_frame = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "                results = pose_model.process(rgb_frame)\n",
    "\t\t\t\t\n",
    "                if results.pose_landmarks:\n",
    "                    # Flatten 33 landmarks × 4 = 132 values\n",
    "                    keypoints = []\n",
    "                    for lm in results.pose_landmarks.landmark:\n",
    "                        keypoints.extend([lm.x, lm.y, lm.z, lm.visibility])\n",
    "                    sequence.append(keypoints)\n",
    "                else:\n",
    "                    # If no pose was detected, append a zero vector of length 132\n",
    "                    sequence.append([0]*132)\n",
    "            \n",
    "            frame_index += 1  # increment frame counter\n",
    "    \n",
    "    cap.release()\n",
    "    return np.array(sequence)  # shape (num_frames, 132)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 75 sequences.\n"
     ]
    }
   ],
   "source": [
    "data = np.load(\"extracted_data/pose_dataset_3.npz\", allow_pickle=True)\n",
    "X_sequences = list(data[\"X\"])\n",
    "y_labels = list(data[\"y\"])\n",
    "\n",
    "print(f\"✅ Loaded {len(X_sequences)} sequences.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pad or Truncate Sequences\n",
    "LSTM/GRU networks expect uniform sequence lengths in a batch.\n",
    "\n",
    "We define a function pad_or_truncate_sequence that ensures each sequence has exactly MAX_SEQ_LEN frames.\n",
    "\n",
    "+ If a sequence is longer than MAX_SEQ_LEN, we take the first MAX_SEQ_LEN frames.\n",
    "\n",
    "+ If it’s shorter, we pad with zeros at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_or_truncate_sequence(seq, max_len=30):\n",
    "    \"\"\"\n",
    "    seq: (T, 132) array for T frames\n",
    "    Returns an array of shape (max_len, 132).\n",
    "    \"\"\"\n",
    "    \n",
    "    length = seq.shape[0]\n",
    "    num_features = seq.shape[1]\n",
    "    \n",
    "    # print(length, num_features)\n",
    "    \n",
    "    if length > max_len:\n",
    "        # Truncate\n",
    "        return seq[:max_len, :]\n",
    "    else:\n",
    "        # Pad with zeros\n",
    "        padded = np.zeros((max_len, num_features))\n",
    "        padded[:length, :] = seq\n",
    "        return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence Length Statistics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence_Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>75.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>75.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>79.725260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>33.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>47.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>79.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>338.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sequence_Length\n",
       "count        75.000000\n",
       "mean         75.533333\n",
       "std          79.725260\n",
       "min           1.000000\n",
       "25%          33.000000\n",
       "50%          47.000000\n",
       "75%          79.500000\n",
       "max         338.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate average sequence length from X_sequences\n",
    "seq_lengths = [seq.shape[0] for seq in X_sequences]\n",
    "# Convert sequence lengths to a DataFrame\n",
    "df_seq_lengths = pd.DataFrame(seq_lengths, columns=['Sequence_Length'])\n",
    "\n",
    "# Get statistical description\n",
    "print(\"Sequence Length Statistics:\")\n",
    "df_seq_lengths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final shape of data: (75, 5, 132)\n"
     ]
    }
   ],
   "source": [
    "X_seq_padded = []\n",
    "\n",
    "for seq in X_sequences:\n",
    "    seq_padded = pad_or_truncate_sequence(seq, max_len=MAX_SEQ_LEN)\n",
    "    X_seq_padded.append(seq_padded)\n",
    "\n",
    "X_seq_padded = np.array(X_seq_padded)  # shape => (num_videos, MAX_SEQ_LEN, 132)\n",
    "\n",
    "print(\"Final shape of data:\", X_seq_padded.shape)\n",
    "# Should be (N, 20, 132) if MAX_SEQ_LEN=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write to CSV file for visualization and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Encode Labels and Split Data\n",
    "\n",
    "We use LabelEncoder to convert \"Kicking\", \"Punching\", \"Non-violent\" into numeric IDs: e.g. 0, 1, 2.\n",
    "\n",
    "Then we split into train/test sets (e.g., 80/20) for fair evaluation.\n",
    "\n",
    "We store them as X_train, X_test, y_train, y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (60, 5, 132) Test shape: (15, 5, 132)\n",
      "Train labels shape: (60,) Test labels shape: (15,)\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y_int = label_encoder.fit_transform(y_labels)  \n",
    "# e.g. \"Kick\"->0, \"Punching\"->2, \"Standing\"->1 (the mapping depends on alphabetical order)\n",
    "\n",
    "# Convert to NumPy\n",
    "y_int = np.array(y_int)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_seq_padded, \n",
    "    y_int, \n",
    "    test_size=0.2, \n",
    "    stratify=y_int,  # keep classes balanced\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n",
    "print(\"Train labels shape:\", y_train.shape, \"Test labels shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N: 75 \n",
      "MAX_SEQ_LEN: 5 \n",
      "NUM_FEATURES: 132 \n",
      "num_classes: 3\n"
     ]
    }
   ],
   "source": [
    "N, MAX_SEQ_LEN, NUM_FEATURES = X_seq_padded.shape\n",
    "NUM_CLASSES = len(np.unique(y_int))\n",
    "\n",
    "print(\"N:\", N, \"\\nMAX_SEQ_LEN:\", MAX_SEQ_LEN, \"\\nNUM_FEATURES:\", NUM_FEATURES, \"\\nnum_classes:\", NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of videos in the 'Single_person_violent/Kicking' folder: 23\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "BASE_DIR = \"Single_person_violent\"\n",
    "KICKING_DIR = os.path.join(BASE_DIR, \"Punching\")\n",
    "\n",
    "# Use glob to list all .mp4 or .avi files in the Kicking directory\n",
    "video_paths = glob.glob(os.path.join(KICKING_DIR, \"*.mp4\")) + glob.glob(os.path.join(KICKING_DIR, \"*.avi\"))\n",
    "\n",
    "num_videos = len(video_paths)\n",
    "print(f\"Total number of videos in the 'Single_person_violent/Kicking' folder: {num_videos}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Make Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "path = \"utils/weight/\"\n",
    "lstm_path = path + \"lstm_model.h5\"\n",
    "gru_path = path + \"gru_model.h5\"\n",
    "dnn_pth = path + \"dnn_model.h5\"\n",
    "\n",
    "#Load model\n",
    "MAX_SEQ_LEN = 30\n",
    "lstm_infer_model = tf.keras.models.load_model(lstm_path)\n",
    "gru_infer_model = tf.keras.models.load_model(gru_path)\n",
    "dnn_infer_model = tf.keras.models.load_model(dnn_pth)\n",
    "infer_model = dnn_infer_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# Buffer to store latest MAX_SEQ_LEN frames of keypoints\n",
    "buffer = collections.deque(maxlen=MAX_SEQ_LEN)\n",
    "\n",
    "# Start webcam\n",
    "cap = cv.VideoCapture(0)\n",
    "cap.set(cv.CAP_PROP_FRAME_WIDTH, 960)\n",
    "cap.set(cv.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Resize frame to consistent shape (optional for clarity)\n",
    "    frame = cv.resize(frame, (960, 720))\n",
    "\n",
    "    # Convert to RGB and get pose\n",
    "    rgb = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "    results = pose.process(rgb)\n",
    "\n",
    "    # Extract pose keypoints\n",
    "    if results.pose_landmarks:\n",
    "        keypoints = []\n",
    "        for lm in results.pose_landmarks.landmark:\n",
    "            keypoints.extend([lm.x, lm.y, lm.z, lm.visibility])\n",
    "    else:\n",
    "        keypoints = [0] * NUM_FEATURES\n",
    "\n",
    "    # Add keypoints to buffer\n",
    "    buffer.append(keypoints)\n",
    "\n",
    "    if len(buffer) == MAX_SEQ_LEN:\n",
    "        seq = np.array(buffer)\n",
    "\n",
    "        if infer_model == dnn_infer_model:\n",
    "            seq_input = seq.reshape(1, -1)  # DNN input shape: (1, 660)\n",
    "        else:\n",
    "            seq_input = np.expand_dims(seq, axis=0)  # LSTM/GRU input shape: (1, 5, 132)\n",
    "\n",
    "        pred = infer_model.predict(seq_input, verbose=0)\n",
    "        class_id = np.argmax(pred)\n",
    "        confidence = float(pred[0][class_id]) * 100\n",
    "        class_name = CLASSES[class_id]\n",
    "        \n",
    "        if class_name == \"Standing\":\n",
    "            display_text = f\"{class_name} ({confidence:.1f}%), Non-Violent\"\n",
    "        else:\n",
    "            display_text = f\"{class_name} ({confidence:.1f}%), Violent\"\n",
    "            \n",
    "        # Show prediction on frame\n",
    "        cv.putText(frame, f'Action: {display_text}', (30, 50),\n",
    "                   cv.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 3)\n",
    "    else:\n",
    "        display_text = \"Non-Violent\"\n",
    "        cv.putText(frame, f'Action: {display_text}', (30, 50),\n",
    "                   cv.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 3)\n",
    "\n",
    "    # Draw pose landmarks\n",
    "    if results.pose_landmarks:\n",
    "        mp.solutions.drawing_utils.draw_landmarks(\n",
    "            frame,\n",
    "            results.pose_landmarks,\n",
    "            mp_pose.POSE_CONNECTIONS,\n",
    "            landmark_drawing_spec=mp.solutions.drawing_utils.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=2),\n",
    "            connection_drawing_spec=mp.solutions.drawing_utils.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2)\n",
    "        )\n",
    "\n",
    "    # Display the frame\n",
    "    cv.imshow(\"Live Action Detection\", frame)\n",
    "\n",
    "    # Press 'q' to exit\n",
    "    if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Cleanup\n",
    "cap.release()\n",
    "cv.destroyAllWindows()\n",
    "pose.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: punching5.mp4\n"
     ]
    }
   ],
   "source": [
    "# Use the test_video_paths that's already defined in the context\n",
    "# test_video_path = \"Single_person_violent\\Kicking\\kicking23.avi\"\n",
    "test_video_path = \"Single_person_violent\\Punching\\punching5.mp4\"\n",
    "\n",
    "# test_video_path = \"A-Dataset-for-Automatic-Violence-Detection-in-Videos\\\\violence-detection-dataset\\\\violent\\cam1\\\\1.mp4\" \n",
    "print(f\"Processing: {os.path.basename(test_video_path)}\")\n",
    "\n",
    "# Initialize MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5,\n",
    "    model_complexity=1\n",
    ")\n",
    "\n",
    "# Open video file\n",
    "cap = cv.VideoCapture(test_video_path)\n",
    "\n",
    "# Buffer to store latest MAX_SEQ_LEN frames of keypoints\n",
    "buffer = collections.deque(maxlen=MAX_SEQ_LEN)\n",
    "\n",
    "while True:\n",
    "\tret, frame = cap.read()\n",
    "\tif not ret:\n",
    "\t\tbreak\n",
    "\n",
    "\t# Convert to RGB and get pose\n",
    "\trgb = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "\tresults = pose.process(rgb)\n",
    "\n",
    "\t# Extract pose keypoints\n",
    "\tif results.pose_landmarks:\n",
    "\t\tkeypoints = []\n",
    "\t\tfor lm in results.pose_landmarks.landmark:\n",
    "\t\t\tkeypoints.extend([lm.x, lm.y, lm.z, lm.visibility])\n",
    "\telse:\n",
    "\t\tkeypoints = [0] * NUM_FEATURES\n",
    "\n",
    "\t# Add keypoints to buffer\n",
    "\tbuffer.append(keypoints)\n",
    "\n",
    "\tif len(buffer) == MAX_SEQ_LEN:\n",
    "\t\tseq = np.array(buffer)\n",
    "\n",
    "\t\tif infer_model == dnn_infer_model:\n",
    "\t\t\tseq_input = seq.reshape(1, -1)  # DNN input shape: (1, 660)\n",
    "\t\telse:\n",
    "\t\t\tseq_input = np.expand_dims(seq, axis=0)  # LSTM/GRU input shape: (1, 5, 132)\n",
    "\n",
    "\t\tpred = infer_model.predict(seq_input, verbose=0)\n",
    "\t\tclass_id = np.argmax(pred)\n",
    "\t\tconfidence = float(pred[0][class_id]) * 100\n",
    "\t\tclass_name = CLASSES[class_id]\n",
    "\n",
    "\t\tif class_name == \"Standing\":\n",
    "\t\t\tdisplay_text = f\"{class_name} ({confidence:.1f}%), Non-Violent\"\n",
    "\t\telse:\n",
    "\t\t\tdisplay_text = f\"{class_name} ({confidence:.1f}%), Violent\"\n",
    "\n",
    "\t\t# Show prediction on frame\n",
    "\t\tcv.putText(frame, f'Action: {display_text}', (30, 50),\n",
    "\t\t\t\t   cv.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 3)\n",
    "\n",
    "\t# Draw pose landmarks\n",
    "\tif results.pose_landmarks:\n",
    "\t\tmp.solutions.drawing_utils.draw_landmarks(\n",
    "\t\t\tframe,\n",
    "\t\t\tresults.pose_landmarks,\n",
    "\t\t\tmp_pose.POSE_CONNECTIONS,\n",
    "\t\t\tlandmark_drawing_spec=mp.solutions.drawing_utils.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=2),\n",
    "\t\t\tconnection_drawing_spec=mp.solutions.drawing_utils.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2)\n",
    "\t\t)\n",
    "\n",
    "\t# Display the frame\n",
    "\tcv.imshow(f\"Action Detection - {os.path.basename(test_video_path)}\", frame)\n",
    "\n",
    "\t# Press 'q' to exit\n",
    "\tif cv.waitKey(1) & 0xFF == ord('q'):\n",
    "\t\tbreak\n",
    "\n",
    "# Cleanup\n",
    "cap.release()\n",
    "cv.destroyAllWindows()\n",
    "pose.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediapipe_pose",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
